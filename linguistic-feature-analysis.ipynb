{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ9BHdJDNHR9dZENzwEV09",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysavine/tweepfakes/blob/main/linguistic-feature-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linguistic Features Analysis**\n",
        "\n",
        "Data\n",
        "*   https://arxiv.org/abs/2008.00036 (Tweepfakes)\n",
        "\n",
        "\n",
        "Resources for Linguistic Feature Extraction\n",
        "*   https://github.com/sherbold/chatgpt-student-essay-study/blob/v1.1/calc_linguistic_features.ipynb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IzAwJuL9l_Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-Processing**"
      ],
      "metadata": {
        "id": "QPDq7LE1kUjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexicalrichness\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "R6Qo01HsvK7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKwjYwfOji6W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy.interpolate import make_interp_spline\n",
        "from collections import Counter\n",
        "from lexicalrichness import LexicalRichness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Dataframes\n",
        "test = pd.read_csv('test.csv', sep=';')\n",
        "train = pd.read_csv('train.csv', sep=';')\n",
        "valid = pd.read_csv('validation.csv', sep=';')"
      ],
      "metadata": {
        "id": "NRPVYys1ubqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove titles or additional less relevant portions of texts\n",
        "\n",
        "# Current format of TweepFakes : screen_name;text;account.type;class_type\n",
        "    # Username\n",
        "    # Tweet or Tweep\n",
        "    # Human or bot\n",
        "    # Type of bot\n",
        "\n",
        "# For Tweepfakes - drop username (0), drop type of bot (3)\n",
        "df_list = [test, train, valid]\n",
        "for df in df_list:\n",
        "  df = df.drop(df.columns[[0, 3]], axis=1)"
      ],
      "metadata": {
        "id": "mJLx6cTUj02v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER4arSFQczsS",
        "outputId": "2f065f44-8c7a-4728-e4d7-21ed78243942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2558, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB7P5Bh1dAzz",
        "outputId": "6038d8aa-d8dc-4d1f-8c01-2894ba681c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20712, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhg6qqFjdCfn",
        "outputId": "967acdc9-e4dc-4757-9f04-9a951d35b32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2302, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesses data with spaCy for later use\n",
        "\n",
        "# Tweets - preprocess with spaCy\n",
        "\n",
        "test[\"tw_spacy\"] = test[\"text\"].apply(lambda x: nlp(x))\n",
        "test[\"tw_lemma\"] = test[\"tw_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))\n",
        "\n",
        "train[\"tw_spacy\"] = train[\"text\"].apply(lambda x: nlp(x))\n",
        "train[\"tw_lemma\"] = train[\"tw_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))\n",
        "\n",
        "valid[\"tw_spacy\"] = valid[\"text\"].apply(lambda x: nlp(x))\n",
        "valid[\"tw_lemma\"] = valid[\"tw_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))\n",
        "\n",
        "# Type of account - check to make sure that entries are only 'human' or 'bot'\n",
        "\n",
        "# Print all unique values or variations in the specified column\n",
        "unique_values = train['account.type'].unique()\n",
        "\n",
        "# Display the unique values\n",
        "for value in unique_values:\n",
        "    print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTVMI1eDVU_p",
        "outputId": "625e8efd-be63-42ff-ff60-a4faa42e5b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bot\n",
            "human\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linguistic Features Analysis\n",
        "\n",
        "I will only be looking at the training data for linguistic features analysis."
      ],
      "metadata": {
        "id": "lNwIa8EjcsjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the \"human\" and \"bot\" data into separate dfs\n",
        "\n",
        "# Filtering the DataFrame based on the entries in the 2nd column\n",
        "bot_df = train[train['account.type'] == 'bot']\n",
        "human_df = train[train['account.type'] == 'human']"
      ],
      "metadata": {
        "id": "J3yYCufidh10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentence Count**"
      ],
      "metadata": {
        "id": "VLyYpB3zYner"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of sentences\n",
        "\n",
        "def num_of_sent(text):\n",
        "    i = 0\n",
        "    for sentence in text:\n",
        "        i += 1\n",
        "    return i\n",
        "\n",
        "bot_df[\"bot_sent_count\"] = bot_df[\"text\"].apply(lambda x: num_of_sent(x))\n",
        "human_df[\"human_sent_count\"] = human_df[\"text\"].apply(lambda x: num_of_sent(x))"
      ],
      "metadata": {
        "id": "Iek7z5jTkGAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2732b1d9-144d-42fc-dba1-e046a5c29583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-bbd4d94f7d88>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_sent_count\"] = bot_df[\"text\"].apply(lambda x: num_of_sent(x))\n",
            "<ipython-input-23-bbd4d94f7d88>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_sent_count\"] = human_df[\"text\"].apply(lambda x: num_of_sent(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Count**"
      ],
      "metadata": {
        "id": "Ir31S536cG5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words\n",
        "\n",
        "def num_of_words(text):\n",
        "    count = len(text.split())\n",
        "    return count\n",
        "\n",
        "bot_df[\"bot_word_count\"] = bot_df[\"text\"].apply(lambda x: num_of_words(x))\n",
        "human_df[\"human_word_count\"] = human_df[\"text\"].apply(lambda x: num_of_words(x))"
      ],
      "metadata": {
        "id": "CY6uo8KIkJHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac629a1-6f72-49d5-bbfd-88acf298f810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f07f7e56cf6f>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_word_count\"] = bot_df[\"text\"].apply(lambda x: num_of_words(x))\n",
            "<ipython-input-24-f07f7e56cf6f>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_word_count\"] = human_df[\"text\"].apply(lambda x: num_of_words(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentence Complexity**"
      ],
      "metadata": {
        "id": "lcC9lIGUkJqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcualtes the number of specified dependency label within a sentence\n",
        "def calculate_dep_score(text):\n",
        "    temp = []\n",
        "    for sentence in nlp(text).sents:\n",
        "        temp.append(sent_complexity_structure(sentence))\n",
        "    return np.mean(temp)\n",
        "\n",
        "# Return the number of specified dependency labels found\n",
        "def sent_complexity_structure(doc):\n",
        "    return len([token for token in doc if (token.dep_ == \"acl\" or token.dep_ == \"conj\" or token.dep_ == \"advcl\"or token.dep_ == \"ccomp\"\n",
        "    or token.dep_ == \"csubj\" or token.dep_ == \"discourse\" or token.dep_ == \"parataxis\")])\n",
        "\n",
        "# Calculates the dependency depth\n",
        "def calculate_dep_length(text):\n",
        "    temp = []\n",
        "    for sentence in nlp(text).sents:\n",
        "        temp.append(walk_tree(sentence.root, 0))\n",
        "    return np.mean(temp)\n",
        "\n",
        "# Walks the dependency tree and returns the depth\n",
        "def walk_tree(node, depth):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return max(walk_tree(child, depth + 1) for child in node.children)\n",
        "    else:\n",
        "        return depth\n",
        "\n",
        "\n",
        "bot_df[\"bot_sent_complex_tags\"] = bot_df[\"text\"].apply(lambda x: calculate_dep_score(x))\n",
        "human_df[\"human_sent_complex_tags\"] = human_df[\"text\"].apply(lambda x: calculate_dep_score(x))\n",
        "\n",
        "bot_df[\"bot_sent_complex_depth\"] = bot_df[\"text\"].apply(lambda x: calculate_dep_length(x))\n",
        "human_df[\"human_sent_complex_depth\"] = human_df[\"text\"].apply(lambda x: calculate_dep_length(x))"
      ],
      "metadata": {
        "id": "Sx8Ch_iPkhXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a59aad1-3469-4971-d66d-5782c5d1e944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-4c6ee7a87f07>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_sent_complex_tags\"] = bot_df[\"text\"].apply(lambda x: calculate_dep_score(x))\n",
            "<ipython-input-31-4c6ee7a87f07>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_sent_complex_tags\"] = human_df[\"text\"].apply(lambda x: calculate_dep_score(x))\n",
            "<ipython-input-31-4c6ee7a87f07>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_sent_complex_depth\"] = bot_df[\"text\"].apply(lambda x: calculate_dep_length(x))\n",
            "<ipython-input-31-4c6ee7a87f07>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_sent_complex_depth\"] = human_df[\"text\"].apply(lambda x: calculate_dep_length(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lexical Diversity**"
      ],
      "metadata": {
        "id": "ihlStNUekghp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates MTLD score for the whole essay\n",
        "\n",
        "def calculate_lex_richness_MTLD2(text):\n",
        "    lex = LexicalRichness(text)\n",
        "    if hasattr(lex, 'words') and lex.words > 0:\n",
        "        lex_rich_score = lex.mtld(threshold=0.72)\n",
        "        return lex_rich_score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "bot_df[\"bot_LD\"] = bot_df[\"text\"].apply(lambda x: calculate_lex_richness_MTLD2(x))\n",
        "human_df[\"human_LD\"] = human_df[\"text\"].apply(lambda x: calculate_lex_richness_MTLD2(x))"
      ],
      "metadata": {
        "id": "aD6wxjjUkLx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb8f7f4-ee2c-47c1-ae48-457d58fb8bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-d29a80890781>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_LD\"] = bot_df[\"text\"].apply(lambda x: calculate_lex_richness_MTLD2(x))\n",
            "<ipython-input-26-d29a80890781>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_LD\"] = human_df[\"text\"].apply(lambda x: calculate_lex_richness_MTLD2(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Discourse Markers**"
      ],
      "metadata": {
        "id": "y1yE3kiDj3B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Counts the number of modals from the list of modals\n",
        "\n",
        "# modals = pd.read_csv(\"markers/modals.csv\", sep=\",\", encoding=\"UTF-8\", header=None)\n",
        "# modals[0] = modals[0].apply(lambda x: x.replace('_', ' '))\n",
        "\n",
        "# # Counts the number of modals per essay\n",
        "# def count_total_modals(text):\n",
        "#     counter = 0\n",
        "#     for modal in modals.itertuples():\n",
        "#         if modal[1] in text:\n",
        "#             counter += text.count(modal[1])\n",
        "#     return counter\n",
        "\n",
        "# bot_df[\"bot_modals1\"] = bot_df[\"train_lemma\"].apply(lambda x: count_total_modals(x))\n",
        "# human_df[\"human_modals1\"] = human_df[\"train_lemma\"].apply(lambda x: count_total_modals(x))\n"
      ],
      "metadata": {
        "id": "1N2F6TVEkYfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Counts the number of modals using POS tagging\n",
        "\n",
        "# bot_df[\"bot_pos\"] = bot_df[\"tw_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
        "# human_df[\"human_pos\"] = human_df[\"tw_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
        "\n",
        "# bot_df[\"bot_modals2\"] = bot_df[\"bot_pos\"].str.count(r'MD')\n",
        "# human_df[\"bot_modals2\"] = human_df[\"human_pos\"].str.count(r'MD')"
      ],
      "metadata": {
        "id": "NtN_mTPMkgD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculates total number of modals per essay\n",
        "\n",
        "# bot_df[\"bot_modals_all\"] = bot_df[\"bot_modals2\"] + bot_df[\"bot_modals1\"]\n",
        "# human_df[\"human_modals_all\"] = human_df[\"human_modals2\"] + human_df[\"human_modals1\"]"
      ],
      "metadata": {
        "id": "BKvExZgYkxsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Epistemic Markers**"
      ],
      "metadata": {
        "id": "v-Ljm_52m49y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts the total number of epistemic markers per essay\n",
        "\n",
        "def find_epistemic_markers(text):\n",
        "    ep_markers = []\n",
        "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)(?:\\s\\w+)?(?:\\s\\w+)?\\s(?:believes?|thinks?|means?|worry|worries|know|guesse?s?|assumes?)\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"(?:It|it)\\sis\\s(?:believed|known|assumed|thought)\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"(?:I|We|we)\\s(?:am|are)\\s(?:thinking|guessing)\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)(?:\\s\\w+)?\\s(?:do|does)\\snot\\s(?:believe?|think|know)\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)\\swould(?:\\s\\w+)?(?:\\snot)?\\ssay\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"I\\sam\\s(?:afraid|sure|confident)\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"(?:My|my|Our|our)\\s(?:experience|opinion|belief|knowledge|worry|worries|concerns?|guesse?s?)\\s(?:is|are)\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"[In]n\\s(?:my|our)(?:\\s\\w+)?\\sopinion\", text))\n",
        "    ep_markers.extend(re.findall(r\"As\\sfar\\sas\\s(?:I|We|we)\\s(?:am|are)\\sconcerned\", text))\n",
        "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)\\s(?:can|could|may|might)(?:\\s\\w+)?\\sconclude\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"I\\s(?:am\\swilling\\sto|must)\\ssay\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"One\\s(?:can|could|may|might)\\ssay\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"[Oo]ne\\s(?:can|could|may|might)\\ssay\\s(?:that)?\", text))\n",
        "    ep_markers.extend(re.findall(r\"[Ii]t\\sis\\s(?:obvious|(?:un)?clear)\", text))\n",
        "    ep_markers.extend(re.findall(r\"[Ii]t\\s(?:seems|feels|looks)\", text))\n",
        "    return len(ep_markers)\n",
        "\n",
        "bot_df[\"bot_EpMarkers\"] = bot_df[\"text\"].apply(lambda x: find_epistemic_markers(x))\n",
        "human_df[\"human_EpMarkers\"] = human_df[\"text\"].apply(lambda x: find_epistemic_markers(x))"
      ],
      "metadata": {
        "id": "eztNp2UZm4py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72da4322-c2f7-41b0-d0a3-7fe94d0f2313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-a009bf85c8aa>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_EpMarkers\"] = bot_df[\"text\"].apply(lambda x: find_epistemic_markers(x))\n",
            "<ipython-input-27-a009bf85c8aa>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_EpMarkers\"] = human_df[\"text\"].apply(lambda x: find_epistemic_markers(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nomilisations**"
      ],
      "metadata": {
        "id": "idOo2rIlS62-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts the total number of nominalisations per essay\n",
        "\n",
        "def nominalisation_counter(text):\n",
        "    suffixes_n = r'\\b[A-Z]*\\w+(?:tion|ment|ance|ence|ion|it(?:y|ies)|ness|ship)(?:s|es)?\\b'\n",
        "\n",
        "    nom_nouns = []\n",
        "    nouns = [token.text for token in text if token.pos_ == 'NOUN']\n",
        "    nom_nouns = [noun for noun in nouns if re.match(suffixes_n, noun)]\n",
        "\n",
        "    return(len(nom_nouns))\n",
        "\n",
        "bot_df[\"bot_nominalisation\"] = bot_df[\"tw_spacy\"].apply(lambda x: nominalisation_counter(x))\n",
        "human_df[\"human_nominalisation\"] = human_df[\"tw_spacy\"].apply(lambda x: nominalisation_counter(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHQ91wNIS6Rr",
        "outputId": "1a09cd40-de0c-4f15-b1f3-426deb7d95a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-c9fd67c52e1d>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_nominalisation\"] = bot_df[\"tw_spacy\"].apply(lambda x: nominalisation_counter(x))\n",
            "<ipython-input-28-c9fd67c52e1d>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_nominalisation\"] = human_df[\"tw_spacy\"].apply(lambda x: nominalisation_counter(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts the average number of features (discourse markers, modals, epistemic markers, nominalisations) per sentence for each essay\n",
        "\n",
        "def average_per_sentence(feature, sent):\n",
        "    average = feature/sent\n",
        "    return(average)\n",
        "\n",
        "#bot_df[\"bot_dm_per_sent\"] = bot_df.apply(lambda row: average_per_sentence(row[\"bot_discourse\"], row[\"bot_sent_count\"]), axis=1)\n",
        "#human_df[\"human_dm_per_sent\"] = human_df.apply(lambda row: average_per_sentence(row[\"human_discourse\"], row[\"human_sent_count\"]), axis=1)\n",
        "\n",
        "#bot_df[\"bot_mod_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"bot_modals_all\"], row[\"bot_sent_count\"]), axis=1)\n",
        "#human_df[\"human_mod_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"human_modals_all\"], row[\"human_sent_count\"]), axis=1)\n",
        "\n",
        "bot_df[\"bot_ep_per_sent\"] = bot_df.apply(lambda row: average_per_sentence(row[\"bot_EpMarkers\"], row[\"bot_sent_count\"]), axis=1)\n",
        "human_df[\"human_ep_per_sent\"] = human_df.apply(lambda row: average_per_sentence(row[\"human_EpMarkers\"], row[\"human_sent_count\"]), axis=1)\n",
        "\n",
        "bot_df[\"bot_nom_per_sent\"] = bot_df.apply(lambda row: average_per_sentence(row[\"bot_nominalisation\"], row[\"bot_sent_count\"]), axis=1)\n",
        "human_df[\"human_nom_per_sent\"] = human_df.apply(lambda row: average_per_sentence(row[\"human_nominalisation\"], row[\"human_sent_count\"]), axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJESyM6aTCcY",
        "outputId": "cfaa875a-38bf-4b89-b30c-307bd196c6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-9e8154a9542e>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_ep_per_sent\"] = bot_df.apply(lambda row: average_per_sentence(row[\"bot_EpMarkers\"], row[\"bot_sent_count\"]), axis=1)\n",
            "<ipython-input-29-9e8154a9542e>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_ep_per_sent\"] = human_df.apply(lambda row: average_per_sentence(row[\"human_EpMarkers\"], row[\"human_sent_count\"]), axis=1)\n",
            "<ipython-input-29-9e8154a9542e>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  bot_df[\"bot_nom_per_sent\"] = bot_df.apply(lambda row: average_per_sentence(row[\"bot_nominalisation\"], row[\"bot_sent_count\"]), axis=1)\n",
            "<ipython-input-29-9e8154a9542e>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  human_df[\"human_nom_per_sent\"] = human_df.apply(lambda row: average_per_sentence(row[\"human_nominalisation\"], row[\"human_sent_count\"]), axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Print Results**"
      ],
      "metadata": {
        "id": "LlFDQZxFUemf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence complexity based on a number of certain dependency tags\")\n",
        "print(\"bot:\", np.mean(bot_df[\"bot_sent_complex_tags\"]))\n",
        "print(\"human: \", np.mean(human_df[\"human_sent_complex_tags\"]), \"\\n\")\n",
        "\n",
        "print(\"Sentence complexity based on the tree depth\")\n",
        "print(\"bot:\", np.mean(bot_df[\"bot_sent_complex_depth\"]))\n",
        "print(\"human: \", np.mean(human_df[\"human_sent_complex_depth\"]), \"\\n\")\n",
        "\n",
        "print(\"MTLD lexical diversity score\")\n",
        "print(\"bot:\", np.mean(bot_df[\"bot_LD\"]))\n",
        "print(\"human: \", np.mean(human_df[\"human_LD\"]), \"\\n\")\n",
        "\n",
        "# print(\"Average number of discourse markers per essay\")\n",
        "# print(\"bot:\", np.mean(bot_df[\"bot_discourse\"]))\n",
        "# print(\"human: \", np.mean(human_df[\"human_discourse\"]), \"\\n\")\n",
        "\n",
        "# print(\"Average number of modals (from the list) per essay\")\n",
        "# print(\"bot:\", np.mean(bot_df[\"bot_modals1\"]))\n",
        "# print(\"human: \", np.mean(human_df[\"human_modals1\"]), \"\\n\")\n",
        "\n",
        "# print(\"Average number of modals (POS-tags) per essay\")\n",
        "# print(\"bot:\", np.mean(bot_df[\"bot_modals2\"]))\n",
        "# print(\"human: \", np.mean(human_df[\"human_modals2\"]), \"\\n\")\n",
        "\n",
        "print(\"Average number of epistemic markers per essay\")\n",
        "print(\"bot:\", np.mean(bot_df[\"bot_EpMarkers\"]))\n",
        "print(\"human: \", np.mean(human_df[\"human_EpMarkers\"]), \"\\n\")\n",
        "\n",
        "print(\"Average number of nominalisations per essay\")\n",
        "print(\"bot:\", np.mean(bot_df[\"bot_nominalisation\"]))\n",
        "print(\"human: \", np.mean(human_df[\"human_nominalisation\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icMNJmzZUhVV",
        "outputId": "58a2173b-245a-48c9-ee9f-0edb1cb2635b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence complexity based on a number of certain dependency tags\n",
            "bot: 1.1799230490980057\n",
            "human:  0.8386741974958812 \n",
            "\n",
            "Sentence complexity based on the tree depth\n",
            "bot: 4.415448563903522\n",
            "human:  3.4726414700058257 \n",
            "\n",
            "MTLD lexical diversity score\n",
            "bot: 34.61954627126504\n",
            "human:  48.19291374067965 \n",
            "\n",
            "Average number of epistemic markers per essay\n",
            "bot: 0.016611937415491596\n",
            "human:  0.01979146553388685 \n",
            "\n",
            "Average number of nominalisations per essay\n",
            "bot: 0.32963106045972573\n",
            "human:  0.3387719636995559\n"
          ]
        }
      ]
    }
  ]
}